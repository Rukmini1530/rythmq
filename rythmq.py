# -*- coding: utf-8 -*-
"""rythmq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sVfbIvfEOcteJAJIb_o3bAY_KwmeHih3
"""

!pip install gtts

!pip install matplotlib-venn
!pip install mediapipe opencv-python fastdtw matplotlib

import tempfile
import os
from google.colab import files
import io
import matplotlib.pyplot as plt
from gtts import gTTS
from IPython.display import Audio, display
import cv2
import mediapipe as mp
import numpy as np
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
from tqdm.notebook import tqdm

# Initialize MediaPipe Pose
mp_pose = mp.solutions.pose
pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# 🔹 Utility: Detect if a file is an image (by extension or OpenCV check)
def is_image_file(path):
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.webp']
    ext = os.path.splitext(path)[1].lower()
    if ext in image_extensions:
        return True
    try:
        img = cv2.imread(path)
        return img is not None
    except:
        return False

# 🔹 Extract keypoints (supports both videos and images)
def extract_keypoints(path, frame_skip=5, max_frames=300):
    keypoints = []

    if is_image_file(path):
        # Handle image input
        frame = cv2.imread(path)
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(rgb)
        if results.pose_landmarks:
            landmarks = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]
            keypoints.append(landmarks)
        print(f"✅ Processed image: {os.path.basename(path)}")
    else:
        # Handle video input
        cap = cv2.VideoCapture(path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        frames_to_process = min(total_frames // frame_skip, max_frames // frame_skip)
        pbar = tqdm(total=frames_to_process, desc=f"Extracting keypoints from {os.path.basename(path)}")

        frame_count = 0
        processed_frames = 0
        while cap.isOpened() and processed_frames < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            if frame_count % frame_skip == 0:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose.process(rgb)
                if results.pose_landmarks:
                    landmarks = [(lm.x, lm.y) for lm in results.pose_landmarks.landmark]
                    keypoints.append(landmarks)
                processed_frames += frame_skip
                pbar.update(1)
            frame_count += 1
        cap.release()
        pbar.close()
        print(f"✅ Processed video: {os.path.basename(path)}")

    if len(keypoints) == 0:
        print(f"⚠️ No keypoints detected in {os.path.basename(path)}.")
    return np.array(keypoints)


# 🔹 Compare two sequences (DTW)
def compare_sequences(ref_kp, user_kp, body_parts):
    # Handle image inputs (1 frame)
    if ref_kp.ndim == 1:
        ref_kp = np.expand_dims(ref_kp, axis=0)
    if user_kp.ndim == 1:
        user_kp = np.expand_dims(user_kp, axis=0)

    part_scores = {}
    frame_distances = {}

    min_frames = min(ref_kp.shape[0], user_kp.shape[0])
    for part, indices in tqdm(body_parts.items(), desc="Comparing body parts"):
        ref_part_seq = ref_kp[:min_frames, indices, :].reshape(min_frames, -1)
        user_part_seq = user_kp[:min_frames, indices, :].reshape(min_frames, -1)
        distance, _ = fastdtw(ref_part_seq, user_part_seq, dist=euclidean)
        score = max(0, 100 - distance * 10)
        part_scores[part] = round(score, 2)

        # Per-frame distances
        per_frame_dist = []
        for i in range(min_frames):
            frame_dist = euclidean(ref_part_seq[i], user_part_seq[i])
            per_frame_dist.append(frame_dist)
        frame_distances[part] = per_frame_dist
    return part_scores, frame_distances


# 🔹 Visualization (works for single images too)
def draw_heatmap(frame, kp_ref, kp_user):
    overlay = frame.copy()
    height, width, _ = frame.shape
    pose_connections = [
        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),
        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),
        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),
        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST),
        (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE),
        (mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.LEFT_ANKLE),
        (mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE),
        (mp_pose.PoseLandmark.RIGHT_KNEE, mp_pose.PoseLandmark.RIGHT_ANKLE)
    ]
    for connection in pose_connections:
        p1_idx, p2_idx = connection[0].value, connection[1].value
        if p1_idx < len(kp_ref) and p2_idx < len(kp_ref):
            rx1, ry1 = kp_ref[p1_idx]
            rx2, ry2 = kp_ref[p2_idx]
            ux1, uy1 = kp_user[p1_idx]
            ux2, uy2 = kp_user[p2_idx]
            color = (0, 255, 0)
            cv2.line(overlay, (int(ux1 * width), int(uy1 * height)),
                     (int(ux2 * width), int(uy2 * height)), color, 2)
    return cv2.addWeighted(overlay, 0.6, frame, 0.4, 0)

# Define body parts
body_parts = {
    "left_arm": [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_ELBOW.value, mp_pose.PoseLandmark.LEFT_WRIST.value],
    "right_arm": [mp_pose.PoseLandmark.RIGHT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_ELBOW.value, mp_pose.PoseLandmark.RIGHT_WRIST.value],
    "left_leg": [mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.LEFT_KNEE.value, mp_pose.PoseLandmark.LEFT_ANKLE.value],
    "right_leg": [mp_pose.PoseLandmark.RIGHT_HIP.value, mp_pose.PoseLandmark.RIGHT_KNEE.value, mp_pose.PoseLandmark.RIGHT_ANKLE.value],
    "torso": [mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_SHOULDER.value,
              mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.RIGHT_HIP.value]
}

# 🔹 Upload files
print("Please upload the reference file (image or video):")
reference_upload = files.upload()

print("Please upload the user file (image or video):")
user_upload = files.upload()

if not reference_upload or not user_upload:
    print("Please upload both reference and user files.")
else:
    with tempfile.NamedTemporaryFile(delete=False) as ref_temp:
        ref_temp.write(next(iter(reference_upload.values())))
        ref_path = ref_temp.name

    with tempfile.NamedTemporaryFile(delete=False) as user_temp:
        user_temp.write(next(iter(user_upload.values())))
        user_path = user_temp.name

    try:
        print("Starting keypoint extraction...")
        ref_kp = extract_keypoints(ref_path)
        user_kp = extract_keypoints(user_path)
        print("✅ Keypoint extraction complete.\n")

        if ref_kp.size == 0 or user_kp.size == 0:
            print("❌ Could not detect a human pose in one or both files.")
        else:
            print("Starting sequence comparison...")
            scores, frame_distances = compare_sequences(ref_kp, user_kp, body_parts)
            print("✅ Comparison complete!\n")

            for part, score in scores.items():
                print(f"{part}: {score}%")
            overall_score = sum(scores.values()) / len(scores)
            print(f"\n⭐ Overall Accuracy: {overall_score:.2f}%")

            # Generate verbal feedback
            feedback_text = "Your performance analysis is as follows: "
            for part, score in scores.items():
                feedback_text += f"{part} scored {score} percent. "
            feedback_text += f"Overall accuracy is {overall_score:.2f} percent."
            tts = gTTS(text=feedback_text, lang='en')
            tts.save("feedback.mp3")
            display(Audio("feedback.mp3"))

            # Visualization for images only
            if is_image_file(ref_path) and is_image_file(user_path):
                frame_ref = cv2.imread(ref_path)
                frame_user = cv2.imread(user_path)
                heatmap = draw_heatmap(frame_user, ref_kp[0], user_kp[0])
                combined = np.hstack((cv2.resize(frame_ref, (400, 400)), cv2.resize(heatmap, (400, 400))))
                plt.imshow(cv2.cvtColor(combined, cv2.COLOR_BGR2RGB))
                plt.axis('off')
                plt.title("Reference vs User Pose Comparison")
                plt.show()

    finally:
        os.remove(ref_path)
        os.remove(user_path)
        print("\nTemporary files cleaned up.")

# === Cell A: create and run Flask app (paste into Colab and run) ===
import os, textwrap, threading, time, tempfile, shutil, sys, subprocess
from IPython.display import display, HTML

APP_CODE = r"""
import os, tempfile, io, shutil
from flask import Flask, request, jsonify, send_file, render_template_string
from flask_cors import CORS
import cv2
import numpy as np
import mediapipe as mp
from scipy.spatial.distance import euclidean
from fastdtw import fastdtw
from gtts import gTTS

mp_pose = mp.solutions.pose
pose_detector = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)

app = Flask(__name__)
CORS(app)

def is_image_file(path):
    ext = os.path.splitext(path)[1].lower()
    return ext in ['.jpg','.jpeg','.png','.bmp','.webp']

def extract_keypoints(path, frame_skip=5, max_frames=300):
    keypoints = []
    if is_image_file(path):
        frame = cv2.imread(path)
        if frame is None: return np.array([])
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose_detector.process(rgb)
        if results.pose_landmarks:
            landmarks = [(lm.x,lm.y) for lm in results.pose_landmarks.landmark]
            keypoints.append(landmarks)
        return np.array(keypoints)
    cap = cv2.VideoCapture(path)
    if not cap.isOpened(): return np.array([])
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
    frames_to_process = max(1, min(total_frames // frame_skip if total_frames>0 else max_frames//frame_skip, max_frames // frame_skip))
    frame_count = 0; processed = 0
    while cap.isOpened() and processed < max_frames:
        ret, frame = cap.read()
        if not ret: break
        if frame_count % frame_skip == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose_detector.process(rgb)
            if results.pose_landmarks:
                landmarks = [(lm.x,lm.y) for lm in results.pose_landmarks.landmark]
                keypoints.append(landmarks)
            processed += 1
        frame_count += 1
    cap.release()
    return np.array(keypoints)

body_parts = {
    "left_arm":[mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_ELBOW.value, mp_pose.PoseLandmark.LEFT_WRIST.value],
    "right_arm":[mp_pose.PoseLandmark.RIGHT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_ELBOW.value, mp_pose.PoseLandmark.RIGHT_WRIST.value],
    "left_leg":[mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.LEFT_KNEE.value, mp_pose.PoseLandmark.LEFT_ANKLE.value],
    "right_leg":[mp_pose.PoseLandmark.RIGHT_HIP.value, mp_pose.PoseLandmark.RIGHT_KNEE.value, mp_pose.PoseLandmark.RIGHT_ANKLE.value],
    "torso":[mp_pose.PoseLandmark.LEFT_SHOULDER.value, mp_pose.PoseLandmark.RIGHT_SHOULDER.value, mp_pose.PoseLandmark.LEFT_HIP.value, mp_pose.PoseLandmark.RIGHT_HIP.value]
}

def compare_sequences(ref_kp, user_kp, body_parts):
    if ref_kp.ndim==1: ref_kp=np.expand_dims(ref_kp,0)
    if user_kp.ndim==1: user_kp=np.expand_dims(user_kp,0)
    if ref_kp.size==0 or user_kp.size==0: return {},{}
    part_scores, frame_distances={},{}
    min_frames = min(ref_kp.shape[0], user_kp.shape[0])
    for part, indices in body_parts.items():
        try:
            ref_seq = ref_kp[:min_frames, indices, :].reshape(min_frames, -1)
            usr_seq = user_kp[:min_frames, indices, :].reshape(min_frames, -1)
        except: continue
        dist, _ = fastdtw(ref_seq, usr_seq, dist=euclidean)
        score = max(0, 100 - dist * 10)
        part_scores[part] = round(score,2)
        frame_distances[part] = [float(euclidean(ref_seq[i], usr_seq[i])) for i in range(min_frames)]
    return part_scores, frame_distances

def draw_heatmap(frame, kp_ref, kp_user):
    overlay = frame.copy(); h,w = frame.shape[:2]
    pose_connections = [
        (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_ELBOW),
        (mp_pose.PoseLandmark.LEFT_ELBOW, mp_pose.PoseLandmark.LEFT_WRIST),
        (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_ELBOW),
        (mp_pose.PoseLandmark.RIGHT_ELBOW, mp_pose.PoseLandmark.RIGHT_WRIST)
    ]
    for conn in pose_connections:
        i1,i2 = conn[0].value, conn[1].value
        if i1<len(kp_user) and i2<len(kp_user):
            ux1,uy1 = kp_user[i1]; ux2,uy2 = kp_user[i2]
            p1=(int(ux1*w),int(uy1*h)); p2=(int(ux2*w),int(uy2*h))
            cv2.line(overlay,p1,p2,(0,255,0),2)
    return cv2.addWeighted(overlay,0.6,frame,0.4,0)

HTML_PAGE = '''
<!doctype html>
<html><head><title>RythmiQ</title></head>
<body style="background:#EADCF8;font-family:sans-serif;padding:20px;">
<h2>💜 RythmiQ Pose & Rhythm Feedback</h2>
<p>Upload reference & user image or video.</p>
<input id="ref" type="file" accept="video/*,image/*"><br><br>
<input id="user" type="file" accept="video/*,image/*"><br><br>
<button onclick="analyze()">Analyze</button>
<pre id="out"></pre>
<script>
async function analyze(){
  let ref=document.getElementById('ref').files[0];
  let user=document.getElementById('user').files[0];
  if(!ref||!user){alert('Upload both');return;}
  let fd=new FormData(); fd.append('reference',ref); fd.append('user',user);
  let r=await fetch('/analyze',{method:'POST',body:fd});
  let j=await r.json();
  document.getElementById('out').innerText=JSON.stringify(j,null,2);
}
</script>
</body></html>
'''

@app.route('/')
def index(): return HTML_PAGE

@app.route('/analyze', methods=['POST'])
def analyze():
    ref=request.files.get('reference'); user=request.files.get('user')
    if not ref or not user: return jsonify({'error':'Missing files'}),400
    tmp=tempfile.mkdtemp()
    refp=os.path.join(tmp,ref.filename); userp=os.path.join(tmp,user.filename)
    ref.save(refp); user.save(userp)
    ref_kp=extract_keypoints(refp); user_kp=extract_keypoints(userp)
    scores,frames=compare_sequences(ref_kp,user_kp,body_parts)
    overall=sum(scores.values())/len(scores) if scores else 0
    fb=" ".join([f"{k}:{v}%" for k,v in scores.items()])+f". Overall:{overall:.2f}%"
    gTTS(fb,lang='en').save(os.path.join(tmp,'fb.mp3'))
    return jsonify({'scores':scores,'overall':overall,'feedback':fb})

if __name__=="__main__":
    app.run(host='0.0.0.0',port=5000)
"""

# write the app file
with open('rythmiq_flask_app.py','w') as f:
    f.write(APP_CODE)
print("✅ Flask app file created")

def run_flask():
    subprocess.Popen([sys.executable, 'rythmiq_flask_app.py'])
threading.Thread(target=run_flask, daemon=True).start()
time.sleep(3)
print("🚀 Flask started on port 5000. Next, run:\n!npx localtunnel --port 5000")

